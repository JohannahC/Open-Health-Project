---
title: "Open Health Project Protocol: dataset build"
date: "`r Sys.Date()`"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

This document outlines the protocol for building OHP's dataset - which offers a way to explore a decade's worth of industry influence on the development of US clinical practice guidelines. The dataset contains all available records of any payments or gifts received by any author of an included guideline, provided they have US-prescribing rights. 

## The protocol

For each guideline, there are 4 broad steps:

>1. Find the guideline  
>2. Extract the author names & relevant metadata 
>3. Use names to identify correct [National Provider Identifier](https://www.cms.gov/Regulations-and-Guidance/Administrative-Simplification/NationalProvIde>ntStand#:~:text=The%20NPI%20is%20a%20unique,financial%20transactions%20adopted%20under%20HIPAA.) (NPI)
>4. Link each author's NPI to [US Open Payments Data](https://openpaymentsdata.cms.gov/)

To illustrate these steps, this document will walk through the data construction for the [2023 Guideline for the Management of Patients with Aneurysmal Subarachnoid Hemorrhage: A Guideline from the American Heart Association/American Stroke Association](https://www.ahajournals.org/doi/epdf/10.1161/STR.0000000000000436). However, keep in mind that the steps outlined here are designed to be completed programmatically at scale. 

### Step 1: find the guideline

The guideline is the starting point of all further data construction. We need it to figure out who the authors are, in order to then identify if and which payment records exist for them in the US Open Payments database. 

**Guidelines can be found in many places including:**

- Directly on the webpage of the professional medical association (ex. [American Diabetes Association](https://diabetesjournals.org/care/issue/46/Supplement_1), [National Comprehensive Cancer Network (NCCN)](https://www.nccn.org/guidelines/nccn-guidelines), [American Heart Association/American College of Cardiology](https://professional.heart.org/en/guidelines-and-statements))
- Guideline databases ([ECRI Guidelines Trust](https://www.ecri.org/solutions/ecri-guidelines-trust), [Guideline Central](https://www.guidelinecentral.com/), [Guidelines International Network](https://g-i-n.net/)) that maintain curated, (though non-comprehensive lists) of guidelines
- Search engines for biomedical literature ([PubMed](https://pubmed.ncbi.nlm.nih.gov/)). Note: these are messy

**A focus on Digital Object Identifiers (DOI)**

Because what we ultimately want from the guideline is a list of all it's authors (and because we're doing this at scale), we're looking for each guideline's DOI, which will allow us to easily extract metadata for the publication programmatically. Therefore, we're focused on optimizing our approach (and thus this protocol) for ways that facilitate batch DOI harvesting. Our starting point in this project is an excel file obtained from the ECRI Guidelines Trust, which contains a list of all ~3,000 guidelines contained in their database. Many, though not all, guideline URLs in this list contain the DOI directly within it. Example: <https://www.ahajournals.org/doi/epdf/10.1161/STR.0000000000000436> and we are able to extract the DOI from the string, and construct a DOI list that can be fed into an API call for the metadata. 

That said, DOIs may not be the best way to organize guidelines and harvest author metadata. The NCCN, for example, does not publish guidelines with DOIs, but their guidelines present other ways of harvesting author names (ie. webscraping). Details on all the unique ways of batch executing Steps 1 & 2 will be described elsewhere. 

For now, let's proceed to step 2 with the DOI from our example guideline:

*10.1161/STR.0000000000000436*


### Step 2: extract the author names and relevant metadata

The goal of Step 2 is to use our DOI to programmatically extract and organize the guideline's metadata, such that we end up with a dataset consisting of the unique authors of our document.

To do so, we run our DOI through the API developed and maintained by Crossref, one of the world's largest DOI registration agencies. Crossref's API allows anyone to access metadata associated with any of **their own DOIs** (important to remember this fact). Luckily, there's an R package (**rcrossref**) developed for accessing it.

These additional **rcrossref** guides are helpful for understanding this step:

- [rcrossref package/Github repository](https://ciakovx.github.io/rcrossref.html#Searching_by_DOI)
- [It's corresponding YouTube tutorial](https://www.youtube.com/watch?v=dy-raTcj0no)

To begin, we load the following packages: 

rcrossref
usethis
listviewer
tidyverse
dplyr
purrr

To adhere to Crossrefs polite-user policy, we need to provide them with an email address when making the API call (you can store it in the ~/.Renviron as follows)

```{r api-setup, eval=FALSE, echo=TRUE, cache=TRUE}
file.edit("~/.Renviron")
usethis::edit_r_environ()
```

Next, we use the function **get_guideline_metadata()** which we wrote for this purpose. As an argument, it takes any dataset that has a column named "doi" in which dois are listed in string form. For us, we'll use our complete guidelines dataset that also contains a doi column, but in theory, you could give it anything that meets the doi column criteria.

**get_guideline_metadata()** does the following:

- takes the 'doi' column in your dataset and turns it into a vector list of dois
- passes this doi list through the Crossref API using rcrossref's function cr_works() 
- stores this in a df and then plucks the data we want (in our case this is called "data")
- unnests the data we want (author names) into their own rows
- cleans the resulting dataframe so that we keep only the columns we want and that all our final columns are also unnested. 

The columns we're interested in keeping are: the DOI, the guideline title, the journal, the date published, given (first) and family (last) names, along with suffix, affiliations, ORCID IDs and urls. 

Afterwards, we have a dataframe which contains all the author data for our guideline (first name, last name, suffix, affiliations, ORCID) along with the above information on the guideline (doi, title, journal, publication date, url). 

Lastly, we write the df to a csv so we can use it in Step 3. 

### Step 3: Use names to identify correct [National Provider Identifier](https://www.cms.gov/Regulations-and-Guidance/Administrative-Simplification/NationalProvIde>ntStand#:~:text=The%20NPI%20is%20a%20unique,financial%20transactions%20adopted%20under%20HIPAA.) (NPI)

The objective of Step 3 is to match each author with the correct unique National Provier Identifier, where an NPI is available. An NPI should be available in all cases where an author has prescribing rights in the US (ie. a doctor, nurse practitioner, pharmacist).

To accurately match, we need to address the challenge of name variability between the name an author publishes under and the name the author registered their NPI with. Consider the fictitious author who publishes under the name "J. Michael Roberts", but who's NPI is registered as "John M. Roberts", or the author who's changed their last name after marriage, but continues to publish under their last name at birth. A simple name search in the NPI registry may mismatch (or miss entirely) these authors. 

To address this, this phase of the protocol necessitates both manual and programmatic work. 

***

**Overview:** this phase of the protocol consists of 3 steps:

1. **OP eligibility assessment**: 
Because there are cases in which a guideline author may not even be eligible to be in the Open Payments database, it's important that we identify and exclude these authors from our name matching step (lest we accidentally match them to someone else's NPI in the registry because they share a name, and unkowingly search the payments data for the wrong person). Authors will not be in the OP data if they: do not have prescribing rights or have never worked as a healthcare provider in the US.

2. **Fetch potential matches using NPPES API**:
The names of eligible authors are queried in the NPPES API, which enables access to the up-to-date list of everyone with an NPI number and relevant identifying information about them (practice location, licensure, medical specialty, other names, and more).

3. **The NPI verification**: 
select name matches are combed manually once again against specified criteria, to ensure the author is paired with the correct NPI 


***

1. **OP eligibility assessment**
This is a manual step that involves checking the publication and each author for:

- a prescribing credential or a degree that would allow someone to sit a licensure exam. These include:
    + MD
    + DO
    + NP
    + DDS
    + DNP
    + PharmD
    + PA
    + CNM
    + CNS
    + CRNA
    + MBBS (this is a UK MD equivalent)
- a US employer
    + as determined by affiliation on publication (in our example, see Appendix 1)
- for cases where there is a non-US employer or affiliation, but there is a prescribing credential:
    + a manual internet search is undertaken to assess if the author could have previously held US licensure:
    + such as: a former US affiliation, medical school, etc

*** 
Non-US author classification checklist: 

- If a = true, b = false and c = true, then “OP eligibility = true”.
- If a = true, b = false and c = false, then “OP eligibility = false”.

***

  
*If an author is OP-eligible, proceed to step 2*


2. **Fetch potential matches using NPPES API**: 
Using the NPPES API, fetch potential NPI matches for the author, *iterating on all possible names.* 
(see more on this - touch base again with jo to get notes on steps taken)

3. **NPI Verification**:

*non-manual*

    - If only one match is returned, assume it is the correct person.
    - If multiple matches are returned, proceed to manual combing.
    
*manual combing*  

For each author with multiple potential NPI matches, manually cross-check the information we know about them against the items in this list where it is available, prioritizing in descending order (order of most reliable information), ensuring at least 3 of the top 4 items match, and only 1 reasonable match remains. In cases of uncertainty, proceed further down the list:

    - Matching credential
    - Matching practice state with state of affiliation/employer on publication
    - Matching taxonomy (medical specialty)
    - Matching middle names, initials, or other names 
    - If more than one match within the same state, check further location details for more accurate address matching
    - If there is a mismatch in practice location or other items on this list, and it is presumed it still may be the correct person, check for other sources linking the author to the NPI (faculty page, author's professional profile, etc.).
    - See if the guideline/publication in question is listed on their faculty or patient website's publication page.
    - Confirm similarities in specialties/research interests between what you can find about the author and the NPI profile
    - Look for other professional affiliations that might help confirm the match
    - Look for matching medical school or other educational information

*author outreach*

If we cannot confidently identify the NPI match, make an attempt to email the person directly to confirm their NPI.

*list unverified matches as NA*

### Step 4: Link each author's NPI to [US Open Payments Data](https://openpaymentsdata.cms.gov/)

The goal of Step 4 is to identify (and create a subset of) all the payment records in the Open Payment's database associated with each NPI on our verified list (finalized in Step 3). The Open Payments data is available for download at:  https://www.cms.gov/OpenPayments/Data/Dataset-Downloads.  

Although there's also an API, at the time of writing it's not very user friendly, so we've opted to download all the raw data available at this link directly. Each "Program Year" download file represents all payment records for a given year, and contains three separate files of interest for us: the general payments file, the research payments file, and the ownership payments file. These are further defined in XXX, but they're generally self-explanatory. The *Covered Recipient Supplement File for All Program Years* (CRSF) contains information - demographic, specialty, license - on all physicians (and non-physician prescribers) that appear in the Open Payments data. 

**Building the payment record dataset**

Because the unique identifier in Open Payments is not the NPI (but the "Profile ID") we'll first use the CRSF to identify the corresponding Profile ID for each NPI we have. Then, we'll use the Profile ID to access the payment records across all the Program Year files and their sub-files (general, research, ownership). 

To start, we need the following raw data files: 

  - the CRSF
  - the list of verified npis
  - the program year payment record files

Next, we'll use the below suite of functions (written for this purpose) in sequential order. The sequential order bit is important, as often one function will take as input the preceding function's output. For starters, we'll just work with one Program Year at a time - 2021. 

1. **clean_verified_npis()**
   - Remove duplicate and missing NPIs from our verified list for ease of use later
2. **clean_physician_supplement()**
   - Remove anyone with a missing NPI in the CRSF and convert the NPI column to a character in the CRSF 
3. **get_profile_ids()**
    - Identify and create the subset of the CRSF for our authors (using profile IDs) which are in the CRSF (and thus in Open Payments)
4. **get_payment_records()** *x3 - run once for each subfile - general, research, owenership*
    - Create a subset of all the payment records affiliated with each profile ID we have in our dataset returned by **get_profile_ids()**

At this stage, we should have all payment records available for each author with an verified NPI on our list - for the year 2021. Becuase we've had to run **get_payment_records()** three times (once for each payment subfile), it means that our authors' payment records are stored in three separate subset files. We want to bind these together to create a single file for 2021, but we need to add 2 columns to each of them first:

- a column for Year
- a column for payment type (general, research, ownership)

We do this using **mutate()** and then we bind the three datasets together, and repeat **get_payment_records()** for all the remaining program years. Once complete, we bind them all together to complete our mega dataset build - and that's that! 

The process is repeated for all guidelines of interest (~1500)


Coming soon: protocols for API & visual dashboard development 
